## 크롤링 기본원리
- 1. 현재 수집할 데이터 사이트 url : "https://finance.naver.com/item/sise_day.nhn?code=352820&page=1"
- 2. headers 추출(딕셔너리 형태로)
- 3. response = requests.get(url,headers=headers)=> 통신
- 4. response.text => 템플릿코드가 잘 넘어왔는지 확인
- 5. html = bs(response.text,"lxml") => 템플릿코드형태로 변환
- 6. temp = html.select("table") => table태그 추출
- 7. table = pd.read.html(str(temp)) => read.html을 이용해서 page내의 값을 DataFrame로 받아옴
- 8. table이라는 리스트에 두개의 값이 가져와짐
- 9. df_temp = table[0] => 리스트 0번째 데이터를 확인해보면 데이터프레임값이 들어있음
- 10. df_day = df_temp.dropna() => 결측치 제거

## url패턴 이용해서 데이터 수집기 만들기
### 일자별 시세를 페이지별로 수집
def get_day_list(item_code, page_no):
    url = f"https://finance.naver.com/item/sise_day.nhn?code={item_code}&page={page_no}" => url 형식에 맞게 포메팅
    headers = {"user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36"
} => 헤더 가져오기
    response = requests.get(url,headers=headers) => 통신
    html = bs(response.text, "lxml") => 템플릿 lxml로 가공
    temp = html.select("table") => table태그 선택
    df_day = pd.read_html(str(temp)) => read.html로 값을 데이터프레임으로 가져옴
    df_day = df_day[0].dropna() => 결측치 제거
    return df_day
 - * time.sleep() => 여러페이지 데이터수집시 요청텀두기(서버부담)
 
 ## 중복 데이터 제거하기
 - drop_duplicates => row들끼리 data를 비교하여 같은값이 있으면 row중 하나를 삭제
 
 ## 제공데이터 json형태로 받기
 - etf_json = response.json()
 - data = etf_json["키값"]["키값2"]
 - 
 ## json데이터를 pandas의 데이터프레임 형태로 만들기
 - df = pd.DataFrame(data)

 ## 파일명 만들기
 import datetime
 today = datetime.datetie.today().strftime("%Y-%m-%d")
 file_name = f"etf-[today].csv" => 'etf-2021-06-24.csv'
 
 ## 파일로 저장하고 불러오기
 -  df.to_csv(file_name, index=False)
 -  pd.read_csv(file_name,dtype=({"itemcode": np.object}) => # itemcode 숫자 앞의 0 이 지워진다면 dtype={"itemcode": np.object} 로 타입을 지정해 주면 문자형태로 읽어옵니다.
